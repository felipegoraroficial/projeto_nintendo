{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db49a9a-3fa1-42e1-8cfb-a5602a87bc7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, LongType, TimestampType \n",
    "from pyspark.sql.functions import concat_ws, col, regexp_replace, when, lit, desc, max, to_date, regexp_extract, count, to_timestamp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0356b024-5262-44fe-b5ac-82fcb879e9bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Repos/felipegoraro@outlook.com/projeto_nintendo/src/config/spark_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "726f08e4-78ac-489f-8f07-2991dd830a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def carregando_ultima_partition(nome_tabela, coluna_particao):\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"codigo\", StringType(), True),\n",
    "        StructField(\"origem\", StringType(), True),\n",
    "        StructField(\"extract\", StringType(), True),\n",
    "        StructField(\"desconto\", StringType(), True),\n",
    "        StructField(\"link\", StringType(), True),\n",
    "        StructField(\"nome\", StringType(), True),\n",
    "        StructField(\"parcelamento\", StringType(), True),\n",
    "        StructField(\"preco\", StringType(), True),\n",
    "        StructField(\"data_ref\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    print(f\"Tentando ler arquivo Delta de: {nome_tabela}\")\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    dbutils = DBUtils(spark) \n",
    "\n",
    "    # 1- Listando arquivos Delta no Volume Bronze\n",
    "    print(f\"\\n--- Verificação do caminho Volume Bronze ---\")\n",
    "    try:\n",
    "        dbutils.fs.ls(nome_tabela)\n",
    "        print(f\"STATUS OK - Caminho '{nome_tabela}' existe. Tentando carregar como Delta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"STATUS ALERTA - O caminho '{nome_tabela}' não existe ou não é acessível. Detalhe: {e}. Gerando um DataFrame vazio.\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        # Em caso de erro, retorne um dataframe vazio com o schema definido anteriormente\n",
    "        return spark.createDataFrame([], schema=schema)\n",
    "    \n",
    "    # 2-  Recuperar partições existentes\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(nome_tabela)\n",
    "    except Exception as e:\n",
    "        print(f\"STATUS ALERTA - Erro ao acessar a tabela '{nome_tabela}': {e}\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        ultima_particao_df = df.select(max(coluna_particao).alias(\"ultima_particao\"))\n",
    "        \n",
    "        ultima_particao = (\n",
    "            ultima_particao_df.first()[\"ultima_particao\"]\n",
    "            if ultima_particao_df.first()\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"STATUS ALERTA - Erro ao obter última partição da tabela '{nome_tabela}': {e}\")\n",
    "\n",
    "    # 3- Tentando ler o arquivo Delta do Volume Bronze\n",
    "    print(f\"\\n--- Verificação da leitura do arquivo Delta ---\")\n",
    "    try:\n",
    "\n",
    "        filtro = f\"{coluna_particao} = '{ultima_particao}'\"\n",
    "\n",
    "        df = spark.read.format(\"delta\").load(nome_tabela) \\\n",
    "                .where(filtro)\n",
    "        print(f\"STATUS OK - Tabela Delta '{nome_tabela}' carregada com sucesso pela partição: {ultima_particao}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Em caso de erro, retorne um dataframe vazio com o schema definido anteriormente\n",
    "        print(f\"STATUS ALERTA - Erro ao carregar a tabela Delta '{nome_tabela}'. Provavelmente não é uma tabela Delta válida ou não contém dados. Detalhe do erro: {e}\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        return spark.createDataFrame([], schema=schema)\n",
    "    \n",
    "    # 4- Verificando se o dataframe está vazio\n",
    "    # Em caso positivo, retorne um dataframe vazio com o schema definido anteriormente\n",
    "    print(f\"\\n--- Verificação se arquivo Delta está vazio ---\")\n",
    "    if df.rdd.isEmpty():\n",
    "        print(f\"STATUS ALERTA - Tabela '{nome_tabela}' foi carregada como Delta VÁLIDA, mas está completamente vazia. Retornando DataFrame vazio com o schema definido.\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        return spark.createDataFrame([], schema=schema)\n",
    "    else:\n",
    "        print(f\"STATUS OK - Tabela Delta '{nome_tabela}' não está vazio.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "# Caminho para a external location do diretório bronze\n",
    "bronze_path = f\"/Volumes/nintendodatabrickspkjgt7_workspace/nintendo/bronze\"\n",
    "\n",
    "# Lendo arquivo Delta do diretório bronze pela ultima partição\n",
    "df = carregando_ultima_partition(bronze_path, \"data_ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e32924d3-0733-469b-8258-2c6cb58b2eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_not_null_value(df, coluna):\n",
    "\n",
    "    print(f\"Iniciando o filtro de valores vazios na coluna: {coluna}\")\n",
    "\n",
    "    dffiltered = df.filter(col(coluna).isNotNull())\n",
    "\n",
    "    qtddotal = df.count()\n",
    "    qtdnotnull = df.filter(col(coluna).isNotNull()).count()\n",
    "    qtdnull = df.filter(col(coluna).isNull()).count()\n",
    "\n",
    "    print(f\"\\nDataframe filtrado, numero de linhas: {qtdnotnull}\")\n",
    "\n",
    "    assert qtddotal == (qtdnull + qtdnotnull), \\\n",
    "    f\"\\nErro na contagem: O total de linhas ({qtddotal}) não é igual à soma de nulos ({qtdnull}) e não nulos ({qtdnotnull}) para a coluna '{coluna}'.\"\n",
    "\n",
    "    print(f\"\\nFiltro ralizado com sucesso\")\n",
    "    print(f\"\\ndf origem {qtddotal} linhas = df filtrado {qtdnotnull} linhas + df não filtrado {qtdnull} linhas\")\n",
    "\n",
    "    return dffiltered\n",
    "\n",
    "dffiltered = filter_not_null_value(df, \"codigo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1298d61-b950-470a-8e8b-e196b9db99f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def define_data_columns(df):\n",
    "\n",
    "    print(f\"Iniciando conversão de colunas com padrões de data/timestamp\")\n",
    "\n",
    "    formato_data_regex = r\"^\\d{4}-\\d{2}-\\d{2}$\"\n",
    "    formato_timestamp_regex = r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}$\"\n",
    "\n",
    "    colunas_string = [coluna for coluna, dtype in df.dtypes if dtype == \"string\"]\n",
    "\n",
    "    print(f\"\\nColunas strings identificadas no dataframe: {colunas_string}\")\n",
    "\n",
    "    for coluna in colunas_string:\n",
    "        df_sem_nulos = df.filter(col(coluna).isNotNull())\n",
    "\n",
    "        # Verifica se há dados na coluna para evitar erro em count() em DF vazio\n",
    "        if df_sem_nulos.count() == 0:\n",
    "            print(f\"Coluna '{coluna}' não possui valores não nulos. Ignorando.\")\n",
    "            continue\n",
    "\n",
    "        match_data_count = df_sem_nulos.filter(regexp_extract(col(coluna), formato_data_regex, 0) != \"\").count()\n",
    "        match_timestamp_count = df_sem_nulos.filter(regexp_extract(col(coluna), formato_timestamp_regex, 0) != \"\").count()\n",
    "        total_count = df_sem_nulos.count()\n",
    "\n",
    "        if match_data_count == total_count:\n",
    "            print(f\"\\nColuna '{coluna}' com padrões de data para a conversão (yyyy-MM-dd).\")\n",
    "            df = df.withColumn(coluna, to_date(col(coluna), \"yyyy-MM-dd\"))\n",
    "            novo_tipo = dict(df.dtypes)[coluna]\n",
    "            assert novo_tipo == \"date\", f\"STATUS ALERTA -  A coluna {coluna} não foi convertida corretamente para 'date'! Tipo atual: {novo_tipo}\"\n",
    "            print(f\"STATUS OK - Coluna '{coluna}' convertida com sucesso, tipo identificado = {novo_tipo}.\")\n",
    "\n",
    "        elif match_timestamp_count == total_count:\n",
    "            print(f\"\\nColuna '{coluna}' com padrões de timestamp para a conversão (yyyy-MM-dd HH:mm).\")\n",
    "            df = df.withColumn(coluna, to_timestamp(col(coluna), \"yyyy-MM-dd HH:mm\"))\n",
    "            novo_tipo = dict(df.dtypes)[coluna]\n",
    "            assert novo_tipo == \"timestamp\", f\"STATUS ALERTA -  A coluna {coluna} não foi convertida corretamente para 'timestamp'! Tipo atual: {novo_tipo}\"\n",
    "            print(f\"STATUS OK - Coluna '{coluna}' convertida com sucesso, tipo identificado = {novo_tipo}.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"\\nColuna '{coluna}' não corresponde a nenhum padrão de data/timestamp conhecido. Ignorando.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_convert_data = define_data_columns(dffiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4bceee7-5333-470b-9bdb-6bc97579b495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def tratar_parcelamento(df):\n",
    "\n",
    "    print(f\"Iniciando o tratamento de coluna referente à 'parcelamento' no dataframe\")\n",
    "\n",
    "    # Extraia o número de parcelas (já tratando nulos)\n",
    "    df_com_parcelas = df.withColumn(\n",
    "        \"numero_parcelas\",\n",
    "        when(col(\"parcelamento\").isNotNull(), regexp_extract(col(\"parcelamento\"), r'(\\d+)x', 1)).otherwise(lit(0))\n",
    "    )\n",
    "    df_com_parcelas = df_com_parcelas.withColumn(\"numero_parcelas\", col(\"numero_parcelas\").cast(LongType()))\n",
    "\n",
    "    # Extraia o valor da prestação (já tratando nulos e convertendo para Double)\n",
    "    df_com_valores = df_com_parcelas.withColumn(\n",
    "        \"valor_prestacao\",\n",
    "        when(col(\"parcelamento\").isNotNull(), regexp_extract(col(\"parcelamento\"), r'R\\$ (\\d+,\\d{2})', 1)).otherwise(lit(\"0\"))\n",
    "    ).withColumn(\n",
    "        \"valor_prestacao\",\n",
    "        when(col(\"valor_prestacao\") != '0',\n",
    "            regexp_replace(col(\"valor_prestacao\"), \",\", \".\").cast(DoubleType())\n",
    "        ).otherwise(lit(0.0))\n",
    "    )\n",
    "\n",
    "    schema = df_com_valores.schema\n",
    "    print(f\"\\nVerificando Schema no dataframe\\n\")\n",
    "\n",
    "    if \"numero_parcelas\" in schema.fieldNames() and schema[\"numero_parcelas\"].dataType == LongType():\n",
    "        print(\"STATUS OK - A coluna 'numero_parcelas' foi criada e tem o tipo correto (bigint).\")\n",
    "    else:\n",
    "        print(\"STATUS ALERTA - A coluna 'numero_parcelas' não foi criada ou tem o tipo incorreto.\")\n",
    "\n",
    "    if \"valor_prestacao\" in schema.fieldNames() and schema[\"valor_prestacao\"].dataType == DoubleType():\n",
    "        print(\"STATUS OK - A coluna 'valor_prestacao' foi criada e tem o tipo correto (double).\")\n",
    "    else:\n",
    "        print(\"STATUS ALERTA - A coluna 'valor_prestacao' não foi criada ou tem o tipo incorreto.\")\n",
    "\n",
    "    # Remova a coluna original \"parcelamento\"\n",
    "    df_final = df_com_valores.drop(\"parcelamento\")\n",
    "\n",
    "    if \"parcelamento\" not in df_final.columns:\n",
    "        print(\"STATUS OK - A coluna 'parcelamento' foi removida com sucesso.\")\n",
    "    else:\n",
    "        print(\"STATUS ALERTA -Erro: A coluna 'parcelamento' ainda está presente.\")\n",
    "\n",
    "    return df_final\n",
    "\n",
    "df_parcelado = tratar_parcelamento(df_convert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a947d57-89ab-4ea4-90e9-320a6e0c0327",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def define_numeric_columns(df):\n",
    "\n",
    "    print(f\"Iniciando conversão de colunas com padrões inteiros/decimais\")\n",
    "\n",
    "    # Regex para identificar valores percentuais e monetários\n",
    "    regex_percentual = re.compile(r\"^\\d+%$\")\n",
    "    regex_monetario = re.compile(r\"^R\\$?\\s?\\d{1,3}(\\.\\d{3})*(,\\d{2})?$\")\n",
    "\n",
    "    # Obtendo colunas de tipo string\n",
    "    colunas_string = [coluna for coluna, dtype in df.dtypes if dtype == \"string\"]\n",
    "    colunas_percentuais = []\n",
    "    colunas_monetarias = []\n",
    "\n",
    "    # Identifica colunas com valores percentuais e monetários\n",
    "    for coluna in colunas_string:\n",
    "        df_sem_nulos = df.filter(col(coluna).isNotNull())\n",
    "        valores_amostra = df_sem_nulos.select(coluna).rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "        if any(bool(regex_percentual.match(str(valor))) for valor in valores_amostra):\n",
    "            print(f\"\\nA coluna '{coluna}' contém valores no formato percentual.\")\n",
    "            colunas_percentuais.append(coluna)\n",
    "\n",
    "        if any(bool(regex_monetario.match(str(valor))) for valor in valores_amostra):\n",
    "            print(f\"\\nA coluna '{coluna}' contém valores no formato monetário.\\n\")\n",
    "            colunas_monetarias.append(coluna)\n",
    "\n",
    "    # Aplica a conversão para valores percentuais\n",
    "    for coluna in colunas_percentuais:\n",
    "        df = df.withColumn(\n",
    "            coluna,\n",
    "            when(\n",
    "                col(coluna).rlike(\"^\\d+%$\"),\n",
    "                (regexp_replace(col(coluna), \"%\", \"\").cast(DoubleType()) / 100)\n",
    "            ).otherwise(col(coluna))\n",
    "        ).withColumn(coluna, col(coluna).cast(DoubleType()))\n",
    "\n",
    "        print(f\"STATUS OK - Coluna {coluna} convertida com sucesso para tipo 'double'.\")\n",
    "\n",
    "    # Aplica a conversão para valores monetários\n",
    "    for coluna in colunas_monetarias:\n",
    "        df = df.withColumn(\n",
    "            coluna,\n",
    "            when(\n",
    "                col(coluna).rlike(\"^R\\\\$?\\\\s?\\\\d{1,3}(\\\\.\\\\d{3})*(,\\\\d{2})?$\"),\n",
    "                regexp_replace(\n",
    "                    regexp_replace(\n",
    "                        regexp_replace(col(coluna), \"R\\\\$\", \"\"), \n",
    "                        \"\\\\.\", \"\" \n",
    "                    ),\n",
    "                    \",\", \".\"  \n",
    "                ).cast(DoubleType())\n",
    "            ).otherwise(col(coluna))\n",
    "        ).withColumn(coluna, col(coluna).cast(DoubleType()))\n",
    "\n",
    "        print(f\"STATUS OK - Coluna {coluna} convertida com sucesso para tipo 'double'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_numeric = define_numeric_columns(df_parcelado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "549d5e33-11a3-4373-aede-345656c657ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def replace_nulls_with_zero(df):\n",
    "    \n",
    "    print(f\"Iniciando preenchimentod e valores nulos em colunas inteiros/decimais\")\n",
    "    \n",
    "    # Identificar colunas numéricas (inteiras e decimais)\n",
    "    numeric_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, (LongType, DoubleType))]\n",
    "\n",
    "    print(f\"\\nColunas numéricas identificadas: {numeric_cols}\")\n",
    "\n",
    "    # Contar valores nulos antes da transformação\n",
    "    null_counts_before = df.select([count(when(col(c).isNull(), c)).alias(c) for c in numeric_cols]).collect()[0].asDict()\n",
    "    print(f\"\\nValores nulos antes da transformação: {null_counts_before}\")\n",
    "\n",
    "    # Substituir valores nulos por 0 nas colunas numéricas\n",
    "    for col_name in numeric_cols:\n",
    "        df = df.withColumn(col_name, when(col(col_name).isNull(), 0).otherwise(col(col_name)))\n",
    "        print(f\"\\nValor nulo na coluna {col_name} alterado para 0\")\n",
    "\n",
    "    # Contar valores nulos depois da transformação\n",
    "    null_counts_after = df.select([count(when(col(c).isNull(), c)).alias(c) for c in numeric_cols]).collect()[0].asDict()\n",
    "    print(f\"\\nValores nulos depois da transformação: {null_counts_after}\\n\")\n",
    "\n",
    "    # Verificar se todas as colunas tiveram seus valores nulos substituídos\n",
    "    for col_name in numeric_cols:\n",
    "        if null_counts_after[col_name] == 0:\n",
    "            print(f\"STATUS OK - Coluna {col_name} foi corretamente preenchida.\")\n",
    "        else:\n",
    "            print(f\"STATUS ALERTA -Coluna {col_name} ainda contém valores nulos!\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_no_null_numeric = replace_nulls_with_zero(df_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd1edf5-4ebf-4b26-a9e0-a8fd5ee3b29f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def replace_nulls_with_hyphen(df):\n",
    "\n",
    "    print(f\"Iniciando preenchimento de valores nulos em colunas strings\")\n",
    "\n",
    "    # Identificar colunas numéricas (inteiras e decimais)\n",
    "    string_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, (StringType))]\n",
    "\n",
    "    print(f\"\\nColunas numéricas identificadas: {string_cols}\")\n",
    "\n",
    "    # Contar valores nulos antes da transformação\n",
    "    null_counts_before = df.select([count(when(col(c).isNull(), c)).alias(c) for c in string_cols]).collect()[0].asDict()\n",
    "    print(f\"\\nValores nulos antes da transformação: {null_counts_before}\")\n",
    "\n",
    "    # Substituir valores nulos por 0 nas colunas numéricas\n",
    "    for col_name in string_cols:\n",
    "        df = df.withColumn(col_name, when(col(col_name).isNull(), '-').otherwise(col(col_name)))\n",
    "        print(f\"\\nValor nulo na coluna {col_name} alterado para '-'\")\n",
    "\n",
    "    # Contar valores nulos depois da transformação\n",
    "    null_counts_after = df.select([count(when(col(c).isNull(), c)).alias(c) for c in string_cols]).collect()[0].asDict()\n",
    "    print(f\"\\nValores nulos depois da transformação: {null_counts_after}\\n\")\n",
    "\n",
    "    # Verificar se todas as colunas tiveram seus valores nulos substituídos\n",
    "    for col_name in string_cols:\n",
    "        if null_counts_after[col_name] == 0:\n",
    "            print(f\"STATUS OK - Coluna {col_name} foi corretamente preenchida.\")\n",
    "        else:\n",
    "            print(f\"STATUS ALERTA - Coluna {col_name} ainda contém valores nulos!\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_no_null_string = replace_nulls_with_hyphen(df_no_null_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d3945b5-0da4-40de-b5a9-8d29e6faa22d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_memory(df, column_name):\n",
    "\n",
    "    print(f\"Iniciando extração de infromações referente a memoria do console\\n\")\n",
    "    \n",
    "    def extract_memory_info(info):\n",
    "\n",
    "        if isinstance(info, str) and info:\n",
    "            padrao = r'(\\d+)\\s*(G[Bb])'\n",
    "            resultado = re.search(padrao, info, re.IGNORECASE)\n",
    "            if resultado:\n",
    "                return resultado.group(0)\n",
    "        return '-'\n",
    "\n",
    "    extrair_memoria_udf = udf(extract_memory_info, StringType())\n",
    "\n",
    "    df = df.withColumn('memoria', extrair_memoria_udf(col(column_name)))\n",
    "\n",
    "    if \"memoria\" in df.columns:\n",
    "        print(\"STATUS OK - A coluna 'memoria' foi criada com sucesso.\")\n",
    "    else:\n",
    "        print(\"STATUS ALERTA - A coluna 'memoria' não foi encontrada no DataFrame. ❌\")\n",
    "\n",
    "\n",
    "    padrao_gb = r'^\\d+\\s*[Gg][Bb]$'\n",
    "\n",
    "    # Conta os registros na coluna 'memoria' que correspondem ao padrão 'XGB' ou 'X GB'\n",
    "    valid_gb_format_count = df.filter(\n",
    "        (col(\"memoria\").isNotNull()) &\n",
    "        (col(\"memoria\") != \"-\") &\n",
    "        (regexp_extract(col(\"memoria\"), padrao_gb, 0) != \"\")\n",
    "    ).count()\n",
    "\n",
    "    # Conta o total de linhas no DataFrame\n",
    "    total_rows = df.count()\n",
    "\n",
    "    print(f\"\\nTotal de linhas no DataFrame: {total_rows}\")\n",
    "    print(f\"Número de registros na coluna 'memoria' com formato 'XGB' ou 'X GB': {valid_gb_format_count}\")\n",
    "\n",
    "    if valid_gb_format_count > 0:\n",
    "        print(\"STATUS OK - A coluna 'memoria' contém valores no formato esperado ('XGB', 'X Gb', 'X gb', etc.)\")\n",
    "        # Opcional: Mostrar algumas linhas para inspecionar os valores\n",
    "        df.filter(regexp_extract(col(\"memoria\"), padrao_gb, 0) != \"\").select(\"nome\", \"memoria\").show(5, truncate=False)\n",
    "    else:\n",
    "        print(\"STATUS ALERTA - A coluna 'memoria' não contém valores no formato esperado.\")\n",
    "        print(\"Verifique a função 'extract_memory' ou os dados de entrada.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_memory_list = extract_memory(df_no_null_string, 'nome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5974e887-2117-43d5-8fda-959337de09df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def condition_like(df, new_column_name, condition_column, pattern):\n",
    "    \n",
    "    \n",
    "    df = df.withColumn(new_column_name, when(col(condition_column).rlike(pattern), 'Sim').otherwise('Nao'))\n",
    "\n",
    "    return df\n",
    "\n",
    "df_type = condition_like(df_memory_list, 'oled', 'nome', '(?i)Oled')\n",
    "df_type = condition_like(df_type, 'lite', 'nome', '(?i)Lite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b11393c3-8853-4561-9902-3b8d33f6f1f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def carregando_tabela_silver(df,delta_table_path):\n",
    "\n",
    "    print(f\"Iniciando o salvamento do DataFrame no formato Delta em: {delta_table_path}\")\n",
    "\n",
    "    try:\n",
    "        # 1- Obter a contagem de linhas ANTES de salvar ---\n",
    "        num_rows_to_save = df.count()\n",
    "        print(f\"Número de linhas no DataFrame a ser salvo: {num_rows_to_save}\")\n",
    "\n",
    "        # 2- Salvar o DataFrame no formato Delta ---\n",
    "        df.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .partitionBy(\"data_ref\") \\\n",
    "                        .save(delta_table_path)\n",
    "\n",
    "        print(f\"DataFrame salvo com sucesso como tabela Delta particionada por 'extract' em: {delta_table_path}\")\n",
    "\n",
    "        # Início das Verificações de Qualidade Pós-Gravação \n",
    "\n",
    "        # 3- Garantir que os dados foram salvos no caminho\n",
    "        print(f\"\\n--- Verificação: Leitura da Tabela Delta Salva ---\")\n",
    "        df_delta_read = spark.read.format(\"delta\").load(delta_table_path)\n",
    "        print(\"Esquema da tabela Delta lida:\")\n",
    "        df_delta_read.printSchema()\n",
    "        num_rows_saved = df_delta_read.count()\n",
    "\n",
    "        if df_delta_read.isEmpty():\n",
    "            print(f\"STATUS ALERTA - A tabela Delta salva em '{delta_table_path}' está vazia ou não pôde ser lida.\")\n",
    "        else:\n",
    "            print(f\"STATUS OK - A tabela Delta foi lida com sucesso de '{delta_table_path}' com {num_rows_saved} linhas recarregadas.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro geral ao salvar ou verificar a tabela Delta: {e}\")\n",
    "\n",
    "\n",
    "# Caminho para a external location do diretório silver\n",
    "silver_path = f\"/Volumes/nintendodatabrickspkjgt7_workspace/nintendo/silver\"\n",
    "\n",
    "# Sobreescrevendo dados particionados no diretório silver\n",
    "carregando_tabela_silver(df_type, silver_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
