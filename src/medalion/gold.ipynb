{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b54db5d-0fa8-4fd8-8da1-f11cfd411716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, LongType \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import when, col, round, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d27edd04-3b14-4e41-9b66-24b858bfff8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_cdi_today():\n",
    "    url = \"https://brasilapi.com.br/api/taxas/v1/cdi\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        data = response.json()\n",
    "        cdi = 1 + (data['valor'] / 100)\n",
    "        return cdi\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro ao conectar à API da Brasil API: {e}\")\n",
    "        return None\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"Erro ao processar os dados da API: {e}\")\n",
    "        return None\n",
    "\n",
    "cdi_data = get_cdi_today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "405ae017-88fa-4a84-9b32-936d9fe47aca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def carregando_ultima_partition(nome_tabela, coluna_particao):\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"codigo\", StringType(), True),\n",
    "        StructField(\"origem\", StringType(), True),\n",
    "        StructField(\"extract\", StringType(), True),\n",
    "        StructField(\"desconto\", StringType(), True),\n",
    "        StructField(\"link\", StringType(), True),\n",
    "        StructField(\"nome\", StringType(), True),\n",
    "        StructField(\"parcelamento\", StringType(), True),\n",
    "        StructField(\"preco\", StringType(), True),\n",
    "        StructField(\"data_ref\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    print(f\"Tentando ler arquivo Delta de: {nome_tabela}\")\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    dbutils = DBUtils(spark) \n",
    "\n",
    "    # 1- Listando arquivos Delta no Volume Bronze\n",
    "    print(f\"\\n--- Verificação do caminho Volume Bronze ---\")\n",
    "    try:\n",
    "        dbutils.fs.ls(nome_tabela)\n",
    "        print(f\"STATUS OK - Caminho '{nome_tabela}' existe. Tentando carregar como Delta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"STATUS ALERTA - O caminho '{nome_tabela}' não existe ou não é acessível. Detalhe: {e}. Gerando um DataFrame vazio.\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        # Em caso de erro, retorne um dataframe vazio com o schema definido anteriormente\n",
    "        return spark.createDataFrame([], schema=schema)\n",
    "\n",
    "    # 2- Tentando ler o arquivo Delta do Volume Bronze\n",
    "    print(f\"\\n--- Verificação da leitura do arquivo Delta ---\")\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(nome_tabela)\n",
    "        print(f\"STATUS OK - Tabela Delta '{nome_tabela}' carregada com sucesso.\")\n",
    "    except Exception as e:\n",
    "        # Em caso de erro, retorne um dataframe vazio com o schema definido anteriormente\n",
    "        print(f\"STATUS ALERTA - Erro ao carregar a tabela Delta '{nome_tabela}'. Provavelmente não é uma tabela Delta válida ou não contém dados. Detalhe do erro: {e}\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        return spark.createDataFrame([], schema=schema)\n",
    "    \n",
    "    # 3- Verificando se o dataframe está vazio\n",
    "    # Em caso positivo, retorne um dataframe vazio com o schema definido anteriormente\n",
    "    print(f\"\\n--- Verificação se arquivo Delta está vazio ---\")\n",
    "    if df.rdd.isEmpty():\n",
    "        print(f\"STATUS ALERTA - Tabela '{nome_tabela}' foi carregada como Delta VÁLIDA, mas está completamente vazia. Retornando DataFrame vazio com o schema definido.\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        return spark.createDataFrame([], schema=schema)\n",
    "    else:\n",
    "        print(f\"STATUS OK - Tabela Delta '{nome_tabela}' não está vazio.\")\n",
    "\n",
    "    # 4- Buscando a ultima partição do dataframe\n",
    "    print(f\"\\n--- Verificando ultima partição ---\")\n",
    "    ultima_particao_df = df.select(max(coluna_particao).alias(\"ultima_particao\"))\n",
    "    ultima_particao = None\n",
    "    if ultima_particao_df.first():\n",
    "        print(f\"STATUS OK - Tabela Delta '{nome_tabela}' contém partição: {ultima_particao_df}.\")\n",
    "        ultima_particao = ultima_particao_df.first()[\"ultima_particao\"]\n",
    "\n",
    "    print(f\"\\n--- Filtrando Tabela pela ultima partição ---\")\n",
    "    # 5- Iniciando filtro da ultima partição da tabeça\n",
    "    if ultima_particao is not None:\n",
    "        filtro = f\"{coluna_particao} = '{ultima_particao}'\"\n",
    "        df_ultima_particao = df.where(filtro)\n",
    "        print(f\"STATUS OK - Tabela '{nome_tabela}' filtrada pela última partição: {ultima_particao}\")\n",
    "\n",
    "        # Realizando contagem de linhas do dataframe filtrado\n",
    "        qtd = df_ultima_particao.count()\n",
    "\n",
    "        # Se o dataframe estiver vazio, ou seja, menor que 1 linha, retorne um erro pois o dataframe não deve estar vazio\n",
    "        assert qtd > 0, f\"A última partição '{ultima_particao}' da tabela '{nome_tabela}' está vazia.\"\n",
    "        \n",
    "        print(f\"Leitura da tabela '{nome_tabela}' carregada com sucesso. Número de linhas: {qtd}\")\n",
    "\n",
    "        return df_ultima_particao\n",
    "    # Em caso de nenhuma partição encontrada no dataframe, será retornado um dataframe vazio com o schema definido anteriormente\n",
    "    else:\n",
    "        print(f\"STATUS ALERTA - Não foram encontradas partições na tabela '{nome_tabela}'.\")\n",
    "        return spark.createDataFrame([], schema=df.schema)\n",
    "\n",
    "# Caminho para a external location do diretório silver\n",
    "silver_path = f\"/Volumes/nintendodatabrickspkjgt7_workspace/nintendo/silver\"\n",
    "\n",
    "# Lendo arquivo Delta do diretório bronze pela ultima partição\n",
    "df = carregando_ultima_partition(silver_path, \"data_ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6602281-0148-4cc2-a1b0-aa2b4c4719c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def recomendation_price(df, cdi_data):\n",
    "\n",
    "    df_price_desconto = df.withColumn(\"preco_desconto\", round(col(\"preco\") - (col(\"preco\") * col(\"desconto\")), 2))\n",
    "\n",
    "    df_price_parcelado = df_price_desconto.withColumn(\"preco_parcelado\", round((col(\"valor_prestacao\") * col(\"numero_parcelas\")), 2))\n",
    "\n",
    "    df_save_desconto = df_price_parcelado.withColumn(\"save_desconto\", round((col(\"preco\") - col(\"preco_desconto\")), 2))\n",
    "\n",
    "    df_save_parcelado = df_save_desconto.withColumn(\n",
    "        \"save_parcelado\",\n",
    "        when((col(\"preco\") - col(\"preco_parcelado\")) * cdi_data <= 0, 0)\n",
    "        .otherwise(round(((col(\"preco\") - col(\"preco_parcelado\")) * cdi_data), 2))\n",
    "    )\n",
    "\n",
    "    df_pct_parcelado = df_save_parcelado.withColumn(\n",
    "        \"pct_preco_parcelado\",\n",
    "        round(100 -((col(\"preco_parcelado\") / col(\"preco\")) * 100), 2)\n",
    "    )\n",
    "\n",
    "    df_recomendacao = df_pct_parcelado.withColumn(\n",
    "        \"recomendacao\",\n",
    "        when(col(\"desconto\") < col(\"pct_preco_parcelado\"), \"Comprar Parcelado\")\n",
    "        .when(col(\"desconto\") > col(\"pct_preco_parcelado\"), \"Comprar a Vista Pix\")\n",
    "        .otherwise(\"Sem Vantagem Identificada\")\n",
    "    ).drop(\"preco_desconto\", \"preco_parcelado\", \"save_desconto\", \"save_parcelado\", \"pct_preco_parcelado\")\n",
    "\n",
    "    return df_recomendacao\n",
    "\n",
    "df_recomendacao = recomendation_price(df, cdi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b791a1b-672e-498e-8c3c-ce1ccdcffb50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def carregando_tabela_gold(df,delta_table_path):\n",
    "\n",
    "    print(f\"Iniciando o salvamento do DataFrame no formato Delta em: {delta_table_path}\")\n",
    "\n",
    "    try:\n",
    "        # 1- Obter a contagem de linhas ANTES de salvar ---\n",
    "        num_rows_to_save = df.count()\n",
    "        print(f\"Número de linhas no DataFrame a ser salvo: {num_rows_to_save}\")\n",
    "\n",
    "        # 2- Salvar o DataFrame no formato Delta ---\n",
    "        df.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .partitionBy(\"data_ref\") \\\n",
    "                        .save(delta_table_path)\n",
    "\n",
    "        print(f\"DataFrame salvo com sucesso como tabela Delta particionada por 'extract' em: {delta_table_path}\")\n",
    "\n",
    "        # Início das Verificações de Qualidade Pós-Gravação \n",
    "\n",
    "        # 3- Garantir que os dados foram salvos no caminho\n",
    "        print(f\"\\n--- Verificação: Leitura da Tabela Delta Salva ---\")\n",
    "        df_delta_read = spark.read.format(\"delta\").load(delta_table_path)\n",
    "        print(\"Esquema da tabela Delta lida:\")\n",
    "        df_delta_read.printSchema()\n",
    "        num_rows_saved = df_delta_read.count()\n",
    "\n",
    "        if df_delta_read.isEmpty():\n",
    "            print(f\"STATUS ALERTA - A tabela Delta salva em '{delta_table_path}' está vazia ou não pôde ser lida.\")\n",
    "        else:\n",
    "            print(f\"STATUS OK - A tabela Delta foi lida com sucesso de '{delta_table_path}' com {num_rows_saved} linhas recarregadas.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro geral ao salvar ou verificar a tabela Delta: {e}\")\n",
    "\n",
    "\n",
    "# Caminho para a external location do diretório gold\n",
    "gold_path = f\"/Volumes/nintendodatabrickspkjgt7_workspace/nintendo/gold\"\n",
    "\n",
    "# Sobreescrevendo dados particionados no diretório gold\n",
    "carregando_tabela_gold(df_recomendacao, gold_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
