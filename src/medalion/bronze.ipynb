{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d788181-c879-4402-b607-d18255383551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import input_file_name, count, when, col, lit, max,row_number, date_format\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc9df46-275f-4cc9-b9f1-bfa7d3503487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definando o schema da estrutura json\n",
    "json_schema = StructType([\n",
    "    StructField(\"codigo\", StringType(), True),\n",
    "    StructField(\"origem\", StringType(), True),\n",
    "    StructField(\"extract\", StringType(), True),\n",
    "    StructField(\"desconto\", StringType(), True),\n",
    "    StructField(\"link\", StringType(), True),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"parcelamento\", StringType(), True),\n",
    "    StructField(\"preco\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc6c85a-1b91-4852-a5ec-2c8633de3d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def processar_dados_inbound(path_inbound):\n",
    "\n",
    "    print(f\"Tentando ler arquivos JSON de: {path_inbound}\")\n",
    "\n",
    "    try:\n",
    "        # Iniciando a leitura dos arquivos json\n",
    "        # .option(\"multiLine\", \"true\") = Se o arquivo json possuir uma array com várias linhas, será lido como um todo\n",
    "        # .option(\"mode\", \"PERMISSIVE\") = Continua lendo os arquivos JSON mesmo que encontre erros de formatação ou dados que não se encaixam no schema pré definido\n",
    "        # .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") = Adicionando a opção pathGlobFilter para garantir que apenas .json sejam lidos explicitamente\n",
    "        # .withColumn(\"file_source\", input_file_name()) = Adicionando o  nome do caminho de cada arquivo lido em uma coluna\n",
    "        df_json_unificado = spark.read \\\n",
    "                                .option(\"multiLine\", \"true\") \\\n",
    "                                .option(\"mode\", \"PERMISSIVE\") \\\n",
    "                                .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "                                .schema(json_schema) \\\n",
    "                                .json(path_inbound) \\\n",
    "                                .withColumn(\"file_source\", input_file_name()) \n",
    "\n",
    "        print(\"\\n--- Esquema do DataFrame unificado ---\")\n",
    "        df_json_unificado.printSchema()\n",
    "\n",
    "        # 1- Etapa de Qualidade. Verificação de Arquivos Lidos vs Arquivos Esperados\n",
    "        # Usando dbutils para listar arquivos no Volumes\n",
    "        all_files_in_dir =  dbutils.fs.ls(path_inbound)\n",
    "        expected_json_files = [f for f in all_files_in_dir if f.name.endswith(\".json\")]\n",
    "        # Contar quantos arquivos .json existem no Volumes Inbound\n",
    "        num_expected_files = len(expected_json_files)\n",
    "\n",
    "\n",
    "        # Contar quantos arquivos foram realmente processados pela aplicação a partir da coluna criada com o nome de cada arquivo lido\n",
    "        files_processed_df = df_json_unificado.select(\"file_source\").distinct().count()\n",
    "\n",
    "        print(f\"\\n--- Verificação de Arquivos ---\")\n",
    "        print(f\"Número de arquivos .json esperados na pasta '{path_inbound.replace('/*.json', '')}': {num_expected_files}\")\n",
    "        print(f\"Número de arquivos únicos processados no DataFrame: {files_processed_df}\")\n",
    "\n",
    "        if files_processed_df == num_expected_files:\n",
    "            print(\"STATUS: OK - Todos os arquivos JSON esperados foram processados.\")\n",
    "        else:\n",
    "            print(\"STATUS: ALERTA - O número de arquivos processados difere do esperado. Investigue!\")\n",
    "            # Para identificar quais arquivos podem ter sido ignorados:\n",
    "            processed_file_paths = [row.file_source for row in df_json_unificado.select(\"file_source\").distinct().collect()]\n",
    "            expected_file_paths = [f.path for f in expected_json_files]\n",
    "\n",
    "            # Encontrar arquivos esperados que não foram processados\n",
    "            missing_files = [path for path in expected_file_paths if path not in processed_file_paths]\n",
    "            if missing_files:\n",
    "                print(f\"Arquivos JSON esperados que não foram encontrados no DataFrame: {missing_files}\")\n",
    "\n",
    "        # 2 Etapa de Qualidade. Contagem de Registros\n",
    "        #Contar quantidade de linhas no dataframe processado\n",
    "        total_records_loaded = df_json_unificado.count()\n",
    "        print(f\"\\n--- Verificação de Registros ---\")\n",
    "        print(f\"Total de registros carregados no DataFrame: {total_records_loaded}\")\n",
    "\n",
    "        #verificação\n",
    "        if total_records_loaded == 0 and num_expected_files > 0:\n",
    "            print(\"STATUS: ALERTA - Nenhum registro foi carregado, mas há arquivos JSON esperados. Verifique se os arquivos estão vazios ou malformados.\")\n",
    "        elif total_records_loaded > 0:\n",
    "            print(\"STATUS: OK - Registros foram carregados com sucesso.\")\n",
    "        else:\n",
    "            print(\"STATUS: N/A - Nenhuma expectativa de registros (pasta vazia ou sem arquivos .json).\")\n",
    "\n",
    "        # 3- Validação Básica de Dados (Verificar nulos em colunas críticas)\n",
    "        print(f\"\\n--- Verificação de Nulos em Colunas Críticas ---\")\n",
    "        critical_columns = [\"codigo\", \"nome\", \"preco\"] # Exemplo\n",
    "\n",
    "        for col_name in critical_columns:\n",
    "            null_count = df_json_unificado.filter(col(col_name).isNull()).count()\n",
    "            if null_count > 0:\n",
    "                print(f\"ALERTA: Coluna '{col_name}' possui {null_count} valores nulos.\")\n",
    "            else:\n",
    "                print(f\"OK: Coluna '{col_name}' não possui valores nulos.\")\n",
    "\n",
    "        # 4- Verificação de Registros Corrompidos (se houver a coluna _corrupt_record)\n",
    "        if \"_corrupt_record\" in df_json_unificado.columns:\n",
    "            corrupt_records_count = df_json_unificado.filter(col(\"_corrupt_record\").isNotNull()).count()\n",
    "            if corrupt_records_count > 0:\n",
    "                print(f\"\\nALERTA: Encontrados {corrupt_records_count} registros corrompidos no _corrupt_record. Investigue!\")\n",
    "                df_json_unificado.filter(col(\"_corrupt_record\").isNotNull()).select(\"file_source\", \"_corrupt_record\").show(truncate=False)\n",
    "            else:\n",
    "                print(\"\\nOK: Não foram encontrados registros corrompidos na coluna _corrupt_record.\")\n",
    "\n",
    "        # 5- Dropar a coluna 'file_source'\n",
    "        # É importante dropar a coluna *depois* de todas as verificações que a utilizam\n",
    "        df_json_unificado = df_json_unificado.drop(\"file_source\")\n",
    "        print(\"\\n--- Coluna 'file_source' foi removida do DataFrame final. ---\")\n",
    "        print(\"\\nEsquema do DataFrame final:\")\n",
    "        df_json_unificado.printSchema()\n",
    "\n",
    "        return df_json_unificado\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro durante a leitura ou verificação de qualidade: {e}\")\n",
    "\n",
    "# Caminho para a pasta que contém os arquivos JSON\n",
    "json_folder_path = \"/Volumes/nintendodatabrickspkjgt7_workspace/nintendo/inbound/\"\n",
    "\n",
    "df_json_unificado = processar_dados_inbound(json_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09b87969-ef2a-421c-8978-67b737363553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def carregando_delta_full(nome_tabela, coluna_particao):\n",
    "\n",
    "    print(f\"Tentando ler arquivo Delta de: {nome_tabela}\")\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    dbutils = DBUtils(spark) \n",
    "\n",
    "    # 1- Listando arquivos Delta no Volume Bronze\n",
    "    print(f\"\\n--- Verificação do caminho Volume Bronze ---\")\n",
    "    try:\n",
    "        dbutils.fs.ls(nome_tabela)\n",
    "        print(f\"STATUS OK - Caminho '{nome_tabela}' existe. Tentando carregar como Delta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"STATUS ALERTA - O caminho '{nome_tabela}' não existe ou não é acessível. Detalhe: {e}. Gerando um DataFrame vazio.\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        # Em caso de erro, retorne um dataframe vazio com o schema definido anteriormente\n",
    "        return spark.createDataFrame([], schema=json_schema)\n",
    "\n",
    "    # 2- Tentando ler o arquivo Delta do Volume Bronze\n",
    "    print(f\"\\n--- Verificação da leitura do arquivo Delta ---\")\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(nome_tabela)\n",
    "        print(f\"STATUS OK - Tabela Delta '{nome_tabela}' carregada com sucesso.\")\n",
    "    except Exception as e:\n",
    "        # Em caso de erro, retorne um dataframe vazio com o schema definido anteriormente\n",
    "        print(f\"STATUS ALERTA - Erro ao carregar a tabela Delta '{nome_tabela}'. Provavelmente não é uma tabela Delta válida ou não contém dados. Detalhe do erro: {e}\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        return spark.createDataFrame([], schema=json_schema)\n",
    "    \n",
    "    # 3- Verificando se o dataframe está vazio\n",
    "    # Em caso positivo, retorne um dataframe vazio com o schema definido anteriormente\n",
    "    print(f\"\\n--- Verificação se arquivo Delta está vazio ---\")\n",
    "    if df.rdd.isEmpty():\n",
    "        print(f\"STATUS ALERTA - Tabela '{nome_tabela}' foi carregada como Delta VÁLIDA, mas está completamente vazia. Retornando DataFrame vazio com o schema definido.\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        return spark.createDataFrame([], schema=json_schema)\n",
    "    else:\n",
    "        print(f\"STATUS OK - Tabela Delta '{nome_tabela}' não está vazio.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "# Caminho para a external location do diretório bronze\n",
    "bronze_path = f\"/Volumes/nintendodatabrickspkjgt7_workspace/nintendo/bronze\"\n",
    "\n",
    "# Lendo arquivo Delta do diretório bronze pela ultima partição\n",
    "df_old = carregando_delta_full(bronze_path, \"data_ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1accc5cf-8fd3-4437-8c93-ee20fb2ce570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def comparando_dados_incremental(df1, df2):\n",
    "\n",
    "    # 1- Combinando dataframe processado X dataframe bronze\n",
    "    print(f\"\\n--- Unindo dataframes ---\")\n",
    "    if df2.count() > 0:\n",
    "        if \"data_ref\" not in df1.columns:\n",
    "            df1 = df1.withColumn(\"data_ref\", date_format(\"extract\", \"yyyy-MM-dd\"))\n",
    "    df_old_tagged = df2.withColumn(\"source\", lit(\"old\"))\n",
    "    df_new_tagged = df1.withColumn(\"source\", lit(\"new\"))\n",
    "\n",
    "    df_combined = df_new_tagged.unionByName(df_old_tagged)\n",
    "\n",
    "    if df_combined.count() == (df_old_tagged.count() + df_new_tagged.count()):\n",
    "        print(f\"STATUS OK - Dataframe Processado vs Dataframe Bronze unidos com sucesso\")\n",
    "    else:\n",
    "        print(f\"STATUS ALERTA - Dataframes não foram unidos corretamente.\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        return spark.createDataFrame([], schema=json_schema)\n",
    "\n",
    "    # 2- Retirando linhas duplicadas do Dataframe\n",
    "    print(f\"\\n--- Obtendo novos dados ---\")\n",
    "    # Contando ocorrências de cada linha\n",
    "    df_counts = df_combined.groupBy(\"codigo\",\"desconto\", \"parcelamento\", \"preco\").count()\n",
    "    df_counts = df_counts.select('codigo','count')\n",
    "    df_joined = df_combined.join(df_counts.select(\"codigo\", \"count\"), on=\"codigo\", how=\"left\")\n",
    "\n",
    "    # Filtrando apenas as linhas que aparecem uma única vez\n",
    "    df_sem_duplicatas = df_joined.filter(col(\"count\") == 1).drop(\"count\")\n",
    "\n",
    "    window_spec = Window.partitionBy(\"codigo\", \"desconto\", \"link\", \"nome\", \"parcelamento\", \"preco\").orderBy(col(\"extract\").desc())\n",
    "\n",
    "    # Adicionar um número de linha dentro de cada partição\n",
    "    df_ranked = df_sem_duplicatas.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "    # Filtrar para manter apenas a primeira linha (a mais recente) para cada código\n",
    "    df_result = df_ranked.filter(col(\"row_num\") == 1).drop(\"row_num\", \"source\")\n",
    "\n",
    "    # Supondo que sua coluna de data se chame 'extract'\n",
    "    df_result = df_result.withColumn(\"data_ref\", date_format(\"extract\", \"yyyy-MM-dd\"))\n",
    "\n",
    "    # 1. Encontrar a maior data no DataFrame\n",
    "    maior_data = df_result.select(max(\"extract\")).collect()[0][0]\n",
    "\n",
    "    # 2. Filtrar o DataFrame para mostrar apenas as linhas com a maior data\n",
    "    df_maior_data = df_result.filter(col(\"extract\") == maior_data)\n",
    "    \n",
    "    if df_maior_data.count() > 0:\n",
    "        print(f\"STATUS INCREMENTAL - Dataframe possui novos {df_maior_data.count()} dados incremental\")\n",
    "    else:\n",
    "        print(f\"STATUS INCREMENTAL - Dataframes não possuí novos dados.\")\n",
    "\n",
    "    return df_maior_data\n",
    "\n",
    "df_incremental = comparando_dados_incremental(df_json_unificado,df_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49657b2c-b584-4eb9-9ae3-8f3ee79a6959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def carregando_tabela_incremental(df,delta_table_path):\n",
    "\n",
    "    print(f\"Iniciando o salvamento do DataFrame no formato Delta em: {delta_table_path}\")\n",
    "\n",
    "    try:\n",
    "        # 1- Obter a contagem de linhas ANTES de salvar ---\n",
    "        num_rows_to_save = df.count()\n",
    "        df_old = spark.read.format(\"delta\").load(delta_table_path)\n",
    "        num_rows_already_saved = df_old.count()\n",
    "\n",
    "        print(f\"Número de linhas no DataFrame a ser salvo: {num_rows_to_save}\")\n",
    "\n",
    "        # 2- Salvar o DataFrame no formato Delta ---\n",
    "        df.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"append\") \\\n",
    "                        .partitionBy(\"data_ref\") \\\n",
    "                        .save(delta_table_path)\n",
    "\n",
    "        print(f\"DataFrame salvo com sucesso como tabela Delta particionada por 'extract' em: {delta_table_path}\")\n",
    "\n",
    "        # Início das Verificações de Qualidade Pós-Gravação \n",
    "\n",
    "        # 3- Garantir que os dados foram salvos no caminho\n",
    "        print(f\"\\n--- Verificação: Leitura da Tabela Delta Salva ---\")\n",
    "        df_delta_read = spark.read.format(\"delta\").load(delta_table_path)\n",
    "        print(\"Esquema da tabela Delta lida:\")\n",
    "        df_delta_read.printSchema()\n",
    "        print(\"Primeiras 5 linhas da tabela Delta lida:\")\n",
    "        df_delta_read.show(5, truncate=False)\n",
    "\n",
    "        if df_delta_read.isEmpty():\n",
    "            print(f\"STATUS ALERTA - A tabela Delta salva em '{delta_table_path}' está vazia ou não pôde ser lida.\")\n",
    "        else:\n",
    "            print(f\"STATUS OK - A tabela Delta foi lida com sucesso de '{delta_table_path}'.\")\n",
    "\n",
    "\n",
    "        # 4- Verificar se a quantidade de linhas salvas condiz com o que está salvo\n",
    "        row_verify = num_rows_already_saved + num_rows_to_save\n",
    "        df_new = spark.read.format(\"delta\").load(delta_table_path)\n",
    "        qtd_df = df_new.count()\n",
    "        if num_rows_to_save == 0:\n",
    "            print(f\"Sem necessidade de verificação de linhas carregadas, quantidade de novas linhas a serem carregadas igual a: {num_rows_to_save}\")\n",
    "        else:\n",
    "\n",
    "            print(f\"\\n--- Verificação: Contagem de Linhas Salvas ---\")\n",
    "            print(f\"Número de linhas salvas na tabela Delta: {num_rows_to_save}\")\n",
    "\n",
    "            if row_verify == qtd_df:\n",
    "                print(f\"STATUS OK - A quantidade de linhas salvas ({num_rows_to_save}) corresponde à quantidade de linhas no DataFrame original ({row_verify}).\")\n",
    "            else:\n",
    "                print(f\"STATUS ALERTA - A quantidade de linhas salvas ({num_rows_to_save}) NÃO CORRESPONDE à quantidade de linhas no DataFrame original ({row_verify}). Investigue!\")\n",
    "\n",
    "\n",
    "        # 5- Verificar se realmente foi particionado ---\n",
    "        print(f\"\\n--- Verificação: Particionamento por 'data' ---\")\n",
    "\n",
    "        print(\"Conteúdo do diretório Delta (buscando por pastas de partição usando dbutils):\")\n",
    "        try:\n",
    "            # Lista os subdiretórios no caminho Delta. Esperamos ver pastas como data=YYYY-MM-DD\n",
    "            delta_contents = dbutils.fs.ls(delta_table_path)\n",
    "            partition_folders_found = [f.name for f in delta_contents if f.isDir and \"=\" in f.name]\n",
    "            if partition_folders_found:\n",
    "                print(f\"STATUS OK - Pastas de partição detectadas (ex: {', '.join(partition_folders_found[:3])}...):\")\n",
    "            else:\n",
    "                print(\"Nenhuma pasta de partição padrão (ex: 'data=...') detectada diretamente no caminho raiz.\")\n",
    "\n",
    "        except Exception as ls_e:\n",
    "            print(f\"STATUS ALERTA - Erro ao listar conteúdo do diretório Delta com dbutils.fs.ls(): {ls_e}\")\n",
    "\n",
    "\n",
    "        # Verificando por qual coluna a Tabela Delta foi particionada\n",
    "        try:\n",
    "            spark.sql(f\"DESCRIBE DETAIL delta.`{delta_table_path}`\").show(truncate=False)\n",
    "            table_details_df = spark.sql(f\"DESCRIBE DETAIL delta.`{delta_table_path}`\")\n",
    "            partition_columns = table_details_df.select(\"partitionColumns\").collect()[0][0]\n",
    "\n",
    "            if \"data_ref\" in partition_columns:\n",
    "                print(f\"STATUS OK - A tabela Delta está particionada pela coluna 'data_ref'.\")\n",
    "            else:\n",
    "                print(f\"STATUS ALERTA - A tabela Delta NÃO parece estar particionada pela coluna 'data_ref'. Partições encontradas: {partition_columns}\")\n",
    "\n",
    "        except Exception as sql_e:\n",
    "            print(f\"STATUS ALERTA - Não foi possível obter detalhes da tabela Delta (verifique o log): {sql_e}\")\n",
    "            # A linha abaixo não pode ser executada por estar dentro de um try-except, por isso a retirei\n",
    "            # print(\"Tente verificar manualmente a estrutura de pastas com '%fs ls -l dbfs:/nintendo/bronze/'.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro geral ao salvar ou verificar a tabela Delta: {e}\")\n",
    "\n",
    "carregando_tabela_incremental(df_incremental,bronze_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
