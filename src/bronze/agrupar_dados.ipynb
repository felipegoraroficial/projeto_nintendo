{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728ff4dd-fc05-43fa-a174-daec935a1cbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import input_file_name, count, when, col, lit, max,row_number, date_format\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c62f5d-804b-40c9-a4ad-15c36e8ad136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definando o schema da estrutura json\n",
    "json_schema = StructType([\n",
    "    StructField(\"codigo\", StringType(), True),\n",
    "    StructField(\"origem\", StringType(), True),\n",
    "    StructField(\"extract\", StringType(), True),\n",
    "    StructField(\"desconto\", StringType(), True),\n",
    "    StructField(\"link\", StringType(), True),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"parcelamento\", StringType(), True),\n",
    "    StructField(\"preco\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0329d00-2a62-4321-a1c5-e699e82b4543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def processar_dados_inbound(path_inbound):\n",
    "\n",
    "    print(f\"Tentando ler arquivos JSON de: {path_inbound}\")\n",
    "\n",
    "    try:\n",
    "        # Iniciando a leitura dos arquivos json\n",
    "        # .option(\"multiLine\", \"true\") = Se o arquivo json possuir uma array com várias linhas, será lido como um todo\n",
    "        # .option(\"mode\", \"PERMISSIVE\") = Continua lendo os arquivos JSON mesmo que encontre erros de formatação ou dados que não se encaixam no schema pré definido\n",
    "        # .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") = Adicionando a opção pathGlobFilter para garantir que apenas .json sejam lidos explicitamente\n",
    "        # .withColumn(\"file_source\", input_file_name()) = Adicionando o  nome do caminho de cada arquivo lido em uma coluna\n",
    "        df_json_unificado = spark.read \\\n",
    "                                .option(\"multiLine\", \"true\") \\\n",
    "                                .option(\"mode\", \"PERMISSIVE\") \\\n",
    "                                .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "                                .schema(json_schema) \\\n",
    "                                .json(path_inbound) \\\n",
    "                                .withColumn(\"file_source\", input_file_name()) \n",
    "\n",
    "        print(\"\\n--- Esquema do DataFrame unificado ---\")\n",
    "        df_json_unificado.printSchema()\n",
    "\n",
    "        # --- 1 Etapa de Qualidade. Verificação de Arquivos Lidos vs. Arquivos Esperados ---\n",
    "        # Usando dbutils para listar arquivos no DBFS\n",
    "        all_files_in_dir =  dbutils.fs.ls(path_inbound)\n",
    "        expected_json_files = [f for f in all_files_in_dir if f.name.endswith(\".json\")]\n",
    "        # Contar quantos arquivos .json existem na pasta\n",
    "        num_expected_files = len(expected_json_files)\n",
    "\n",
    "\n",
    "        # Contar quantos arquivos foram realmente processados pela aplicação a partir da coluna criada com o nome de cada arquivo lido\n",
    "        files_processed_df = df_json_unificado.select(\"file_source\").distinct().count()\n",
    "\n",
    "        print(f\"\\n--- Verificação de Arquivos ---\")\n",
    "        print(f\"Número de arquivos .json esperados na pasta '{path_inbound.replace('/*.json', '')}': {num_expected_files}\")\n",
    "        print(f\"Número de arquivos únicos processados no DataFrame: {files_processed_df}\")\n",
    "\n",
    "        if files_processed_df == num_expected_files:\n",
    "            print(\"STATUS: OK - Todos os arquivos JSON esperados foram processados.\")\n",
    "        else:\n",
    "            print(\"STATUS: ALERTA - O número de arquivos processados difere do esperado. Investigue!\")\n",
    "            # Para identificar quais arquivos podem ter sido ignorados:\n",
    "            processed_file_paths = [row.file_source for row in df_json_unificado.select(\"file_source\").distinct().collect()]\n",
    "            expected_file_paths = [f.path for f in expected_json_files]\n",
    "\n",
    "            # Encontrar arquivos esperados que não foram processados\n",
    "            missing_files = [path for path in expected_file_paths if path not in processed_file_paths]\n",
    "            if missing_files:\n",
    "                print(f\"Arquivos JSON esperados que não foram encontrados no DataFrame: {missing_files}\")\n",
    "\n",
    "        # --- 2 Etapa de Qualidade. Contagem de Registros ---\n",
    "        #Contar quantidade de linhas no dataframe processado\n",
    "        total_records_loaded = df_json_unificado.count()\n",
    "        print(f\"\\n--- Verificação de Registros ---\")\n",
    "        print(f\"Total de registros carregados no DataFrame: {total_records_loaded}\")\n",
    "\n",
    "        #verificação\n",
    "        if total_records_loaded == 0 and num_expected_files > 0:\n",
    "            print(\"STATUS: ALERTA - Nenhum registro foi carregado, mas há arquivos JSON esperados. Verifique se os arquivos estão vazios ou malformados.\")\n",
    "        elif total_records_loaded > 0:\n",
    "            print(\"STATUS: OK - Registros foram carregados com sucesso.\")\n",
    "        else:\n",
    "            print(\"STATUS: N/A - Nenhuma expectativa de registros (pasta vazia ou sem arquivos .json).\")\n",
    "\n",
    "        # --- 3. Validação Básica de Dados (Verificar nulos em colunas críticas) ---\n",
    "        print(f\"\\n--- Verificação de Nulos em Colunas Críticas ---\")\n",
    "        critical_columns = [\"codigo\", \"nome\", \"preco\"] # Exemplo\n",
    "\n",
    "        for col_name in critical_columns:\n",
    "            null_count = df_json_unificado.filter(col(col_name).isNull()).count()\n",
    "            if null_count > 0:\n",
    "                print(f\"ALERTA: Coluna '{col_name}' possui {null_count} valores nulos.\")\n",
    "            else:\n",
    "                print(f\"OK: Coluna '{col_name}' não possui valores nulos.\")\n",
    "\n",
    "        # --- 4. Verificação de Registros Corrompidos (se houver a coluna _corrupt_record) ---\n",
    "        if \"_corrupt_record\" in df_json_unificado.columns:\n",
    "            corrupt_records_count = df_json_unificado.filter(col(\"_corrupt_record\").isNotNull()).count()\n",
    "            if corrupt_records_count > 0:\n",
    "                print(f\"\\nALERTA: Encontrados {corrupt_records_count} registros corrompidos no _corrupt_record. Investigue!\")\n",
    "                df_json_unificado.filter(col(\"_corrupt_record\").isNotNull()).select(\"file_source\", \"_corrupt_record\").show(truncate=False)\n",
    "            else:\n",
    "                print(\"\\nOK: Não foram encontrados registros corrompidos na coluna _corrupt_record.\")\n",
    "\n",
    "        # --- 5. Dropar a coluna 'file_source' ---\n",
    "        # É importante dropar a coluna *depois* de todas as verificações que a utilizam\n",
    "        df_json_unificado = df_json_unificado.drop(\"file_source\")\n",
    "        print(\"\\n--- Coluna 'file_source' foi removida do DataFrame final. ---\")\n",
    "        print(\"\\nEsquema do DataFrame final:\")\n",
    "        df_json_unificado.printSchema()\n",
    "        print(\"\\nPrimeiras 5 linhas do DataFrame final:\")\n",
    "        df_json_unificado.show(5, truncate=False)\n",
    "\n",
    "        return df_json_unificado\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro durante a leitura ou verificação de qualidade: {e}\")\n",
    "\n",
    "# Caminho para a pasta que contém os arquivos JSON\n",
    "json_folder_path = \"/Volumes/nintendodatabricksp1okle_workspace/nintendo/inbound/\"\n",
    "\n",
    "df_json_unificado = processar_dados_inbound(json_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ea0766-70d7-4e55-a847-3ec14432239c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def carregando_ultima_partition(nome_tabela, coluna_particao):\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    dbutils = DBUtils(spark) \n",
    "\n",
    "    try:\n",
    "\n",
    "        dbutils.fs.ls(nome_tabela)\n",
    "        print(f\"Caminho '{nome_tabela}' existe. Tentando carregar como Delta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"O caminho '{nome_tabela}' não existe ou não é acessível. Detalhe: {e}. Gerando um DataFrame vazio.\")\n",
    "        return spark.createDataFrame([], schema=json_schema)\n",
    "\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(nome_tabela)\n",
    "        print(f\"Tabela Delta '{nome_tabela}' carregada com sucesso.\")\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Erro ao carregar a tabela Delta '{nome_tabela}'. Provavelmente não é uma tabela Delta válida. Detalhe do erro: {e}\")\n",
    "        return spark.createDataFrame([], schema=json_schema)\n",
    "    \n",
    "    if df.rdd.isEmpty():\n",
    "        print(f\"Tabela '{nome_tabela}' foi carregada como Delta VÁLIDA, mas está completamente vazia. Retornando DataFrame vazio com o schema definido.\")\n",
    "        return spark.createDataFrame([], schema=json_schema)\n",
    "\n",
    "    ultima_particao_df = df.select(max(coluna_particao).alias(\"ultima_particao\"))\n",
    "    ultima_particao = ultima_particao_df.first()[\"ultima_particao\"] if ultima_particao_df.first() else None\n",
    "\n",
    "    if ultima_particao is not None:\n",
    "        filtro = f\"{coluna_particao} = '{ultima_particao}'\"\n",
    "        df_ultima_particao = df.where(filtro)\n",
    "        print(f\"Tabela '{nome_tabela}' filtrada pela última partição: {ultima_particao}\")\n",
    "\n",
    "        qtd = df_ultima_particao.count()\n",
    "\n",
    "        assert qtd > 0, f\"A última partição '{ultima_particao}' da tabela '{nome_tabela}' está vazia.\"\n",
    "        \n",
    "        print(f\"Leitura da tabela '{nome_tabela}' carregada com sucesso. Número de linhas: {qtd}\")\n",
    "\n",
    "        return df_ultima_particao\n",
    "    else:\n",
    "        print(f\"Não foram encontradas partições na tabela '{nome_tabela}'.\")\n",
    "        return spark.createDataFrame([], schema=df.schema)\n",
    "\n",
    "# Caminho para a external location do diretório bronze\n",
    "bronze_path = f\"/Volumes/nintendodatabricksp1okle_workspace/nintendo/bronze\"\n",
    "\n",
    "# Lendo arquivo Delta do diretório bronze pela ultima partição\n",
    "df_old = carregando_ultima_partition(bronze_path, \"data_ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec59ae87-77f4-429f-871a-fe4a73bf5f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def comparando_dados_incremental(df1, df2):\n",
    "\n",
    "    if df2.rdd.isEmpty() == False:\n",
    "        if \"data_ref\" not in df1.columns:\n",
    "            df1 = df1.withColumn(\"data_ref\", date_format(\"extract\", \"yyyy-MM-dd\"))\n",
    "    df_old_tagged = df2.withColumn(\"source\", lit(\"old\"))\n",
    "    df_new_tagged = df1.withColumn(\"source\", lit(\"new\"))\n",
    "\n",
    "    df_combined = df_new_tagged.unionByName(df_old_tagged)\n",
    "\n",
    "    # Contando ocorrências de cada linha\n",
    "    df_counts = df_combined.groupBy(\"codigo\",\"desconto\", \"parcelamento\", \"preco\").count()\n",
    "    df_counts = df_counts.select('codigo','count')\n",
    "    df_joined = df_combined.join(df_counts.select(\"codigo\", \"count\"), on=\"codigo\", how=\"left\")\n",
    "\n",
    "    # Filtrando apenas as linhas que aparecem uma única vez\n",
    "    df_sem_duplicatas = df_joined.filter(col(\"count\") == 1).drop(\"count\")\n",
    "\n",
    "    window_spec = Window.partitionBy(\"codigo\", \"desconto\", \"link\", \"nome\", \"parcelamento\", \"preco\").orderBy(col(\"extract\").desc())\n",
    "\n",
    "    # Adicionar um número de linha dentro de cada partição\n",
    "    df_ranked = df_sem_duplicatas.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "    # Filtrar para manter apenas a primeira linha (a mais recente) para cada código\n",
    "    df_result = df_ranked.filter(col(\"row_num\") == 1).drop(\"row_num\", \"source\")\n",
    "\n",
    "    # Supondo que sua coluna de data se chame 'extract'\n",
    "    df_result = df_result.withColumn(\"data_ref\", date_format(\"extract\", \"yyyy-MM-dd\"))\n",
    "\n",
    "    # 1. Encontrar a maior data no DataFrame\n",
    "    maior_data = df_result.select(max(\"extract\")).collect()[0][0]\n",
    "\n",
    "    # 2. Filtrar o DataFrame para mostrar apenas as linhas com a maior data\n",
    "    df_maior_data = df_result.filter(col(\"extract\") == maior_data)\n",
    "\n",
    "    return df_maior_data\n",
    "\n",
    "df_incremental = comparando_dados_incremental(df_json_unificado,df_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6484e767-5abb-42ab-ac62-4342fe2b8b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def carregando_tabela_incremental(df,delta_table_path):\n",
    "\n",
    "    print(f\"Iniciando o salvamento do DataFrame no formato Delta em: {delta_table_path}\")\n",
    "\n",
    "    try:\n",
    "        # --- Passo 1: Obter a contagem de linhas ANTES de salvar ---\n",
    "        num_rows_to_save = df.count()\n",
    "        print(f\"Número de linhas no DataFrame a ser salvo: {num_rows_to_save}\")\n",
    "\n",
    "        # --- Passo 2: Salvar o DataFrame no formato Delta ---\n",
    "        df.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"append\") \\\n",
    "                        .partitionBy(\"data_ref\") \\\n",
    "                        .save(delta_table_path)\n",
    "\n",
    "        print(f\"DataFrame salvo com sucesso como tabela Delta particionada por 'extract' em: {delta_table_path}\")\n",
    "\n",
    "        # --- Início das Verificações de Qualidade Pós-Gravação ---\n",
    "\n",
    "        # --- 1. Garantir que os dados foram salvos no caminho ---\n",
    "        print(f\"\\n--- Verificação: Leitura da Tabela Delta Salva ---\")\n",
    "        df_delta_read = spark.read.format(\"delta\").load(delta_table_path)\n",
    "        print(\"Esquema da tabela Delta lida:\")\n",
    "        df_delta_read.printSchema()\n",
    "        print(\"Primeiras 5 linhas da tabela Delta lida:\")\n",
    "        df_delta_read.show(5, truncate=False)\n",
    "\n",
    "        if df_delta_read.isEmpty():\n",
    "            print(f\"ALERTA: A tabela Delta salva em '{delta_table_path}' está vazia ou não pôde ser lida.\")\n",
    "        else:\n",
    "            print(f\"OK: A tabela Delta foi lida com sucesso de '{delta_table_path}'.\")\n",
    "\n",
    "\n",
    "        # --- 2. Verificar se a quantidade de linhas salvas condiz com o que está salvo ---\n",
    "        num_rows_saved = df_delta_read.count()\n",
    "        print(f\"\\n--- Verificação: Contagem de Linhas Salvas ---\")\n",
    "        print(f\"Número de linhas salvas na tabela Delta: {num_rows_saved}\")\n",
    "\n",
    "        if num_rows_saved == num_rows_to_save:\n",
    "            print(f\"STATUS: OK - A quantidade de linhas salvas ({num_rows_saved}) corresponde à quantidade de linhas no DataFrame original ({num_rows_to_save}).\")\n",
    "        else:\n",
    "            print(f\"ALERTA: A quantidade de linhas salvas ({num_rows_saved}) NÃO CORRESPONDE à quantidade de linhas no DataFrame original ({num_rows_to_save}). Investigue!\")\n",
    "\n",
    "\n",
    "        # --- 3. Verificar se realmente foi particionado ---\n",
    "        print(f\"\\n--- Verificação: Particionamento por 'data' ---\")\n",
    "\n",
    "        # Substituindo %fs ls -l por dbutils.fs.ls()\n",
    "        print(\"Conteúdo do diretório Delta (buscando por pastas de partição usando dbutils):\")\n",
    "        try:\n",
    "            # Lista os subdiretórios no caminho Delta. Esperamos ver pastas como data=YYYY-MM-DD\n",
    "            delta_contents = dbutils.fs.ls(delta_table_path)\n",
    "            partition_folders_found = [f.name for f in delta_contents if f.isDir and \"=\" in f.name]\n",
    "            if partition_folders_found:\n",
    "                print(f\"Pastas de partição detectadas (ex: {', '.join(partition_folders_found[:3])}...):\")\n",
    "                # Opcional: mostrar todas as pastas de partição se for um número pequeno\n",
    "                # for p_folder in partition_folders_found:\n",
    "                #     print(f\"  - {p_folder}\")\n",
    "            else:\n",
    "                print(\"Nenhuma pasta de partição padrão (ex: 'data=...') detectada diretamente no caminho raiz.\")\n",
    "\n",
    "        except Exception as ls_e:\n",
    "            print(f\"ALERTA: Erro ao listar conteúdo do diretório Delta com dbutils.fs.ls(): {ls_e}\")\n",
    "\n",
    "\n",
    "        # O método mais confiável continua sendo usar o DESCRIBE DETAIL\n",
    "        try:\n",
    "            spark.sql(f\"DESCRIBE DETAIL delta.`{delta_table_path}`\").show(truncate=False)\n",
    "            table_details_df = spark.sql(f\"DESCRIBE DETAIL delta.`{delta_table_path}`\")\n",
    "            partition_columns = table_details_df.select(\"partitionColumns\").collect()[0][0] # Pega o primeiro elemento da lista\n",
    "\n",
    "            if \"data_ref\" in partition_columns:\n",
    "                print(f\"STATUS: OK - A tabela Delta está particionada pela coluna 'data_ref'.\")\n",
    "            else:\n",
    "                print(f\"ALERTA: A tabela Delta NÃO parece estar particionada pela coluna 'data_ref'. Partições encontradas: {partition_columns}\")\n",
    "\n",
    "        except Exception as sql_e:\n",
    "            print(f\"ALERTA: Não foi possível obter detalhes da tabela Delta (verifique o log): {sql_e}\")\n",
    "            # A linha abaixo não pode ser executada por estar dentro de um try-except, por isso a retirei\n",
    "            # print(\"Tente verificar manualmente a estrutura de pastas com '%fs ls -l dbfs:/nintendo/bronze/'.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro geral ao salvar ou verificar a tabela Delta: {e}\")\n",
    "\n",
    "carregando_tabela_incremental(df_incremental,bronze_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "agrupar_dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
