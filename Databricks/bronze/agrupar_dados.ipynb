{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42897d1b-d7d1-49f5-abd3-46e7a775501a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import input_file_name, count, when, col, lit\n",
    "\n",
    "# 1. Definir o esquema manualmente\n",
    "json_schema = StructType([\n",
    "    StructField(\"codigo\", StringType(), True),\n",
    "    StructField(\"data\", StringType(), True),\n",
    "    StructField(\"desconto\", StringType(), True),\n",
    "    StructField(\"link\", StringType(), True),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"parcelamento\", StringType(), True),\n",
    "    StructField(\"preco\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Caminho para a pasta que contém os arquivos JSON\n",
    "json_folder_path = \"/Volumes/nintendodatabricks037cbq_workspace/nintendo/inbound/*.json\"\n",
    "\n",
    "\n",
    "print(f\"Tentando ler arquivos JSON de: {json_folder_path}\")\n",
    "\n",
    "try:\n",
    "    # --- Leitura dos arquivos JSON ---\n",
    "    # Adicionando a opção pathGlobFilter para garantir que apenas .json sejam lidos explicitamente\n",
    "    df_json_unificado = spark.read \\\n",
    "                             .option(\"multiLine\", \"true\") \\\n",
    "                             .option(\"mode\", \"PERMISSIVE\") \\\n",
    "                             .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "                             .schema(json_schema) \\\n",
    "                             .json(json_folder_path) \\\n",
    "                             .withColumn(\"file_source\", input_file_name()) # Adiciona coluna com o caminho do arquivo\n",
    "\n",
    "    print(\"\\n--- Esquema do DataFrame unificado ---\")\n",
    "    df_json_unificado.printSchema()\n",
    "\n",
    "    # --- 1. Verificação de Arquivos Lidos vs. Arquivos Esperados ---\n",
    "    # Contar quantos arquivos foram realmente processados pelo Spark\n",
    "    files_processed_df = df_json_unificado.select(\"file_source\").distinct().count()\n",
    "    processed_file_paths = [row.file_source for row in df_json_unificado.select(\"file_source\").distinct().collect()]\n",
    "\n",
    "    # Para obter o número de arquivos esperados diretamente de um Volume,\n",
    "    # a melhor prática é usar a API do Spark para listar os arquivos, já que ela entende os caminhos de Volumes.\n",
    "    try:\n",
    "        # Pega a parte da pasta do json_folder_path (removendo \"/*.json\")\n",
    "        base_folder_path = json_folder_path.replace(\"/*.json\", \"\")\n",
    "        # Lista os arquivos .json na pasta usando Spark para compatibilidade com Volumes\n",
    "        # Isso cria um DataFrame com o caminho de cada arquivo\n",
    "        expected_json_files_df = spark.read.format(\"binaryFile\") \\\n",
    "                                            .option(\"pathGlobFilter\", \"*.json\") \\\n",
    "                                            .load(base_folder_path) \\\n",
    "                                            .select(\"path\")\n",
    "\n",
    "        num_expected_files = expected_json_files_df.count()\n",
    "        expected_file_paths_spark = [row.path for row in expected_json_files_df.collect()]\n",
    "\n",
    "    except Exception as e_list:\n",
    "        print(f\"Não foi possível listar arquivos usando Spark no caminho '{base_folder_path}': {e_list}\")\n",
    "        print(\"Prosseguindo com a contagem apenas dos arquivos processados pelo DataFrame.\")\n",
    "        num_expected_files = -1 # Indica que não foi possível obter o número esperado\n",
    "        expected_file_paths_spark = []\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Verificação de Arquivos ---\")\n",
    "    if num_expected_files != -1:\n",
    "        print(f\"Número de arquivos .json esperados na pasta '{base_folder_path}': {num_expected_files}\")\n",
    "    else:\n",
    "        print(f\"Não foi possível determinar o número esperado de arquivos .json na pasta '{base_folder_path}'.\")\n",
    "    print(f\"Número de arquivos únicos processados no DataFrame: {files_processed_df}\")\n",
    "\n",
    "    if num_expected_files != -1 and files_processed_df == num_expected_files:\n",
    "        print(\"STATUS: OK - Todos os arquivos JSON esperados foram processados.\")\n",
    "    elif num_expected_files != -1 and files_processed_df != num_expected_files:\n",
    "        print(\"STATUS: ALERTA - O número de arquivos processados difere do esperado. Investigue!\")\n",
    "        # Para identificar quais arquivos podem ter sido ignorados:\n",
    "        missing_files = [path for path in expected_file_paths_spark if path not in processed_file_paths]\n",
    "        if missing_files:\n",
    "            print(f\"Arquivos JSON esperados que não foram encontrados no DataFrame: {missing_files}\")\n",
    "    else:\n",
    "        print(\"STATUS: INCONCLUSIVO - Não foi possível comparar o número de arquivos esperados com os processados.\")\n",
    "\n",
    "    # --- 2. Contagem de Registros ---\n",
    "    total_records_loaded = df_json_unificado.count()\n",
    "    print(f\"\\n--- Verificação de Registros ---\")\n",
    "    print(f\"Total de registros carregados no DataFrame: {total_records_loaded}\")\n",
    "\n",
    "    if total_records_loaded == 0 and num_expected_files > 0:\n",
    "        print(\"STATUS: ALERTA - Nenhum registro foi carregado, mas há arquivos JSON esperados. Verifique se os arquivos estão vazios ou malformados.\")\n",
    "    elif total_records_loaded > 0:\n",
    "        print(\"STATUS: OK - Registros foram carregados com sucesso.\")\n",
    "    else:\n",
    "        print(\"STATUS: N/A - Nenhuma expectativa de registros (pasta vazia ou sem arquivos .json).\")\n",
    "\n",
    "    # --- 3. Validação Básica de Dados (Verificar nulos em colunas críticas) ---\n",
    "    print(f\"\\n--- Verificação de Nulos em Colunas Críticas ---\")\n",
    "    critical_columns = [\"codigo\", \"nome\", \"preco\"] # Exemplo\n",
    "\n",
    "    for col_name in critical_columns:\n",
    "        null_count = df_json_unificado.filter(col(col_name).isNull()).count()\n",
    "        if null_count > 0:\n",
    "            print(f\"ALERTA: Coluna '{col_name}' possui {null_count} valores nulos.\")\n",
    "        else:\n",
    "            print(f\"OK: Coluna '{col_name}' não possui valores nulos.\")\n",
    "\n",
    "    # --- 4. Verificação de Registros Corrompidos (se houver a coluna _corrupt_record) ---\n",
    "    if \"_corrupt_record\" in df_json_unificado.columns:\n",
    "        corrupt_records_count = df_json_unificado.filter(col(\"_corrupt_record\").isNotNull()).count()\n",
    "        if corrupt_records_count > 0:\n",
    "            print(f\"\\nALERTA: Encontrados {corrupt_records_count} registros corrompidos no _corrupt_record. Investigue!\")\n",
    "            df_json_unificado.filter(col(\"_corrupt_record\").isNotNull()).select(\"file_source\", \"_corrupt_record\").show(truncate=False)\n",
    "        else:\n",
    "            print(\"\\nOK: Não foram encontrados registros corrompidos na coluna _corrupt_record.\")\n",
    "\n",
    "    # --- 5. Dropar a coluna 'file_source' ---\n",
    "    # É importante dropar a coluna *depois* de todas as verificações que a utilizam\n",
    "    df_json_unificado = df_json_unificado.drop(\"file_source\")\n",
    "    print(\"\\n--- Coluna 'file_source' foi removida do DataFrame final. ---\")\n",
    "    print(\"\\nEsquema do DataFrame final:\")\n",
    "    df_json_unificado.printSchema()\n",
    "    print(\"\\nPrimeiras 5 linhas do DataFrame final:\")\n",
    "    df_json_unificado.show(5, truncate=False)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro durante a leitura ou verificação de qualidade: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a3493f4-d6e3-45b2-9d2a-0ca860ce8eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Certifique-se de que df_json_unificado esteja disponível após as etapas anteriores\n",
    "\n",
    "# Caminho de destino para a tabela Delta\n",
    "delta_table_path = \"/Volumes/nintendodatabricks037cbq_workspace/nintendo/bronze\"\n",
    "\n",
    "print(f\"Iniciando o salvamento do DataFrame no formato Delta em: {delta_table_path}\")\n",
    "\n",
    "try:\n",
    "    # --- Passo 1: Obter a contagem de linhas ANTES de salvar ---\n",
    "    num_rows_to_save = df_json_unificado.count()\n",
    "    print(f\"Número de linhas no DataFrame a ser salvo: {num_rows_to_save}\")\n",
    "\n",
    "    # --- Passo 2: Salvar o DataFrame no formato Delta ---\n",
    "    df_json_unificado.write \\\n",
    "                     .format(\"delta\") \\\n",
    "                     .mode(\"overwrite\") \\\n",
    "                     .partitionBy(\"data\") \\\n",
    "                     .save(delta_table_path)\n",
    "\n",
    "    print(f\"DataFrame salvo com sucesso como tabela Delta particionada por 'data' em: {delta_table_path}\")\n",
    "\n",
    "    # --- Início das Verificações de Qualidade Pós-Gravação ---\n",
    "\n",
    "    # --- 1. Garantir que os dados foram salvos no caminho ---\n",
    "    print(f\"\\n--- Verificação: Leitura da Tabela Delta Salva ---\")\n",
    "    df_delta_read = spark.read.format(\"delta\").load(delta_table_path)\n",
    "    print(\"Esquema da tabela Delta lida:\")\n",
    "    df_delta_read.printSchema()\n",
    "    print(\"Primeiras 5 linhas da tabela Delta lida:\")\n",
    "    df_delta_read.show(5, truncate=False)\n",
    "\n",
    "    if df_delta_read.isEmpty():\n",
    "        print(f\"ALERTA: A tabela Delta salva em '{delta_table_path}' está vazia ou não pôde ser lida.\")\n",
    "    else:\n",
    "        print(f\"OK: A tabela Delta foi lida com sucesso de '{delta_table_path}'.\")\n",
    "\n",
    "\n",
    "    # --- 2. Verificar se a quantidade de linhas salvas condiz com o que está salvo ---\n",
    "    num_rows_saved = df_delta_read.count()\n",
    "    print(f\"\\n--- Verificação: Contagem de Linhas Salvas ---\")\n",
    "    print(f\"Número de linhas salvas na tabela Delta: {num_rows_saved}\")\n",
    "\n",
    "    if num_rows_saved == num_rows_to_save:\n",
    "        print(f\"STATUS: OK - A quantidade de linhas salvas ({num_rows_saved}) corresponde à quantidade de linhas no DataFrame original ({num_rows_to_save}).\")\n",
    "    else:\n",
    "        print(f\"ALERTA: A quantidade de linhas salvas ({num_rows_saved}) NÃO CORRESPONDE à quantidade de linhas no DataFrame original ({num_rows_to_save}). Investigue!\")\n",
    "\n",
    "\n",
    "    # --- 3. Verificar se realmente foi particionado ---\n",
    "    print(f\"\\n--- Verificação: Particionamento por 'data' ---\")\n",
    "\n",
    "    # Substituindo %fs ls -l por dbutils.fs.ls()\n",
    "    print(\"Conteúdo do diretório Delta (buscando por pastas de partição usando dbutils):\")\n",
    "    try:\n",
    "        # Lista os subdiretórios no caminho Delta. Esperamos ver pastas como data=YYYY-MM-DD\n",
    "        delta_contents = dbutils.fs.ls(delta_table_path)\n",
    "        partition_folders_found = [f.name for f in delta_contents if f.isDir and \"=\" in f.name]\n",
    "        if partition_folders_found:\n",
    "            print(f\"Pastas de partição detectadas (ex: {', '.join(partition_folders_found[:3])}...):\")\n",
    "            # Opcional: mostrar todas as pastas de partição se for um número pequeno\n",
    "            # for p_folder in partition_folders_found:\n",
    "            #     print(f\"  - {p_folder}\")\n",
    "        else:\n",
    "            print(\"Nenhuma pasta de partição padrão (ex: 'data=...') detectada diretamente no caminho raiz.\")\n",
    "\n",
    "    except Exception as ls_e:\n",
    "        print(f\"ALERTA: Erro ao listar conteúdo do diretório Delta com dbutils.fs.ls(): {ls_e}\")\n",
    "\n",
    "\n",
    "    # O método mais confiável continua sendo usar o DESCRIBE DETAIL\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE DETAIL delta.`{delta_table_path}`\").show(truncate=False)\n",
    "        table_details_df = spark.sql(f\"DESCRIBE DETAIL delta.`{delta_table_path}`\")\n",
    "        partition_columns = table_details_df.select(\"partitionColumns\").collect()[0][0] # Pega o primeiro elemento da lista\n",
    "\n",
    "        if \"data\" in partition_columns:\n",
    "            print(f\"STATUS: OK - A tabela Delta está particionada pela coluna 'data'.\")\n",
    "        else:\n",
    "            print(f\"ALERTA: A tabela Delta NÃO parece estar particionada pela coluna 'data'. Partições encontradas: {partition_columns}\")\n",
    "\n",
    "    except Exception as sql_e:\n",
    "        print(f\"ALERTA: Não foi possível obter detalhes da tabela Delta (verifique o log): {sql_e}\")\n",
    "        # A linha abaixo não pode ser executada por estar dentro de um try-except, por isso a retirei\n",
    "        # print(\"Tente verificar manualmente a estrutura de pastas com '%fs ls -l dbfs:/nintendo/bronze/'.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro geral ao salvar ou verificar a tabela Delta: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "agrupar_dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
